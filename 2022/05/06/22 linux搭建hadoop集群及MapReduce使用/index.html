
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>linux搭建hadoop集群及MapReduce使用 - 杨的个人博客</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="杨笑源, yangxiaoyuan,"> 
    <meta name="description" content="1 Hadoop分布式存储介绍2 搭建Hadoop集群本次实验将搭建一个含有三节点的hadoop集群。

实验环境：宿主机操作系统: Windows10虚拟机软件：VMware Workstatio,"> 
    <meta name="author" content="Xiaoyuan yang"> 
    <link rel="alternative" href="atom.xml" title="杨的个人博客" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="linux搭建hadoop集群及MapReduce使用 - 杨的个人博客"/>
    <meta name="twitter:description" content="1 Hadoop分布式存储介绍2 搭建Hadoop集群本次实验将搭建一个含有三节点的hadoop集群。

实验环境：宿主机操作系统: Windows10虚拟机软件：VMware Workstatio,"/>
    
    
    
    
    <meta property="og:site_name" content="杨的个人博客"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="linux搭建hadoop集群及MapReduce使用 - 杨的个人博客"/>
    <meta property="og:description" content="1 Hadoop分布式存储介绍2 搭建Hadoop集群本次实验将搭建一个含有三节点的hadoop集群。

实验环境：宿主机操作系统: Windows10虚拟机软件：VMware Workstatio,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

<meta name="generator" content="Hexo 6.0.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">杨的个人博客</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">linux搭建hadoop集群及MapReduce使用</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">linux搭建hadoop集群及MapReduce使用</h1>
        <div class="stuff">
            <span>五月 06, 2022</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/Ubuntu/" rel="tag">Ubuntu</a></li></ul>


        </div>
        <div class="content markdown">
            <h2 id="1-Hadoop分布式存储介绍"><a href="#1-Hadoop分布式存储介绍" class="headerlink" title="1 Hadoop分布式存储介绍"></a>1 Hadoop分布式存储介绍</h2><h2 id="2-搭建Hadoop集群"><a href="#2-搭建Hadoop集群" class="headerlink" title="2 搭建Hadoop集群"></a>2 搭建Hadoop集群</h2><p>本次实验将搭建一个含有三节点的hadoop集群。</p>
<blockquote>
<p>实验环境：<br>宿主机操作系统: Windows10<br>虚拟机软件：VMware Workstation<br>虚拟机操作系统1：Ubuntu2004LTS<br>虚拟机操作系统2：Ubuntu2004LTS<br>虚拟机操作系统3：Ubuntu2004LTS</p>
</blockquote>
<h3 id="2-1-创建用户（节点）并配置节点间的免密认证"><a href="#2-1-创建用户（节点）并配置节点间的免密认证" class="headerlink" title="2.1 创建用户（节点）并配置节点间的免密认证"></a>2.1 创建用户（节点）并配置节点间的免密认证</h3><p>在每个节点上分别进行如下操作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建新用户hadoop</span></span><br><span class="line">root@hadoop1:~$ adduser hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将hadoop添加到root用户组里</span></span><br><span class="line">root@hadoop1:~$ chmod-v u+w /etc/sudoers</span><br><span class="line">root@hadoop1:~$ vi /etc/sudoers</span><br><span class="line">在root    ALL=(ALL)       ALL下添加一行：</span><br><span class="line">hadoop    ALL=(ALL)       ALL</span><br><span class="line">root@hadoop1:~$ <span class="built_in">chmod</span> -v u-w /etc/sudoers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置免密认证</span></span><br><span class="line">root@hadoop1:~$ su - hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将.ssh文件夹复制到每个节点</span></span><br><span class="line">hadoop@hadoop1:~$ sudo <span class="built_in">cp</span> -r /root/.ssh ./</span><br><span class="line">hadoop@hadoop1:~$ scp -r ./.ssh hadoop2:~/</span><br><span class="line">hadoop@hadoop1:~$ scp -r ./.ssh hadoop3:~/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改权限</span></span><br><span class="line">hadoop@hadoop1:~$ <span class="built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br><span class="line">hadoop@hadoop1:~$ <span class="built_in">chmod</span> 600 ~/.ssh/config</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>若能通过ssh在每个节点的hadoop用户之间无密钥切换，则配置成功。</p>
<h3 id="2-2-每个节点安装Java和Hadoop"><a href="#2-2-每个节点安装Java和Hadoop" class="headerlink" title="2.2 每个节点安装Java和Hadoop"></a>2.2 每个节点安装Java和Hadoop</h3><p>在每个节点的hadoop用户下分卸进行如下操作</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Java</span></span><br><span class="line">hadoop@hadoop1:~$ wget http://bigdata.cg.lzu.edu.cn/bigdata_software/jdk-8u321-linux-x64.tar.gz</span><br><span class="line">hadoop@hadoop1:~$ tar -zxvf jdk-8u321-linux-x64.tar.gz</span><br><span class="line">hadoop@hadoop1:~$ vi ~/.bashrc</span><br><span class="line">添加如下内容：</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=~/jdk1.8.0_321</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/jre/lib:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"></span><br><span class="line">hadoop@hadoop1:~$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line">hadoop@hadoop1:~$ java -version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装Hadoop</span></span><br><span class="line">hadoop@hadoop1:~$ wget http://bigdata.cg.lzu.edu.cn/bigdata_software/hadoop-3.2.3.tar.gz</span><br><span class="line">hadoop@hadoop1:~$ tar -zxvf hadoop-3.2.3.tar.gz</span><br><span class="line">hadoop@hadoop1:~$ vi ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=~/hadoop-3.2.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=~/hadoop-3.2.3</span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=~/hadoop-3.2.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=hadoop</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=hadoop</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=hadoop</span><br><span class="line"></span><br><span class="line">hadoop@hadoop1:~$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line">hadoop@hadoop1:~$ hadoop version</span><br></pre></td></tr></table></figure>

<h3 id="2-3-配置Hadoop"><a href="#2-3-配置Hadoop" class="headerlink" title="2.3 配置Hadoop"></a>2.3 配置Hadoop</h3><p>在hadoop1节点上配置hadoop：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hadoop@hadoop1:~$ <span class="built_in">cd</span> ~/hadoop-3.2.3/etc/hadoop/</span><br><span class="line"></span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi hadoop-env.sh</span><br><span class="line">添加：</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/hadoop/jdk1.8.0_321</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/hadoop/hadoop-3.2.3</span><br><span class="line"></span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi core-site.xml</span><br><span class="line">添加下方内容</span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi hdfs-site.xml</span><br><span class="line">添加下方内容</span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi mapred-site.xml</span><br><span class="line">添加下方内容</span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi yarn-site.xml</span><br><span class="line">添加下方内容</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用主机名或IP地址配置集群所有节点</span></span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ vi workers</span><br><span class="line">修改为：</span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将hadoop文件夹拷贝到其他节点</span></span><br><span class="line">hadoop@hadoop1:~/hadoop-3.2.3/etc/hadoop/$ <span class="built_in">cd</span> ~</span><br><span class="line">hadoop@hadoop1:~$ scp -r ~/hadoop-3.2.3 hadoop2:~/</span><br><span class="line">hadoop@hadoop1:~$ scp -r ~/hadoop-3.2.3 hadoop3:~/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化文件系统</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>core-site.xml中主要指定了文件系统默认访问地址和端口，以及hdfs文件系统默认目录，内容:<br>！！注意其中hadoop1应该为IP地址</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hdfs服务地址和端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hadoop文件系统目录，namenode、secondarynamenode和data默认都在此目录下，默认为/tmp/hadoop-$&#123;user.name&#125;--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop-3.2.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml配置主要指定namenode、secondarynamenode节点和服务端口，以及副本数和安全检查：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hdfs namenode的http地址，默认为0.0.0.0:9870 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--hdfs secondarynamenode的http地址，默认为0.0.0.0:9868，如需将secondarynamenode放在其他节点，修改主机名即可 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- HDFS副本数，默认为3，单节点伪集群需设置为1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 是否启用hdfs权限检查 ，默认为true开启的，设置为false关闭 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 文件分块大小，默认128MB --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>128m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>mapred-site.xml配置主要指定了MR框架以及一些环境变量和路径：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.admin.user.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/home/hadoop/hadoop-3.2.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/home/hadoop/hadoop-3.2.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>yarn-site.xml，主要指定了yarn resourcemanager节点和一些其他变量：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h3 id="2-4-启动Hadoop"><a href="#2-4-启动Hadoop" class="headerlink" title="2.4 启动Hadoop"></a>2.4 启动Hadoop</h3><p>启动集群服务：（仅在Hadoop1节点上运行即可）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动HDFS</span></span><br><span class="line">hadoop@hadoop1:~$ start-dfs.sh</span><br><span class="line"><span class="comment"># 启动yarn：ResourceManager和NodeManager服务进程</span></span><br><span class="line">hadoop@hadoop1:~$ start-yarn.sh</span><br><span class="line"><span class="comment"># 启动MapReduce JobHistory Server历史服务器和timelineserver时间线服务器</span></span><br><span class="line">hadoop@hadoop1:~$ mapred --daemon start historyserver</span><br><span class="line">hadoop@hadoop1:~$ yarn --daemon start timelineserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看运行节点情况</span></span><br><span class="line">hadoop@hadoop1:~$  jps</span><br></pre></td></tr></table></figure>
<p>也可以通过网址访问node节点：<a target="_blank" rel="noopener" href="http://hadoop1:9870/">http://hadoop1:9870/</a><br>访问NodeManager：<a target="_blank" rel="noopener" href="http://hadoop1:8088/">http://hadoop1:8088/</a><br>访问yarn：<a target="_blank" rel="noopener" href="http://hadoop1:19888/">http://hadoop1:19888/</a></p>
<p>Hadoop上的一些操作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看系统健康状况</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs fsck /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建HDFS文件系统用户目录</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -<span class="built_in">mkdir</span> /user</span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -<span class="built_in">mkdir</span> /user/hadoop/</span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -<span class="built_in">ls</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建空文件</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs –touchz /directory/filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件大小</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs –<span class="built_in">du</span> –s /directory/filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件内容</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs –<span class="built_in">cat</span> /path/to/file_in_hdfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件从本地上传</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -copyFromLocal &lt;localsrc&gt; &lt;hdfs destination&gt;</span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -put &lt;localsrc&gt; &lt;destination&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件下载到本地</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -copyToLocal &lt;hdfs <span class="built_in">source</span>&gt; &lt;localdst&gt;</span><br><span class="line">hadoop@hadoop1:~$  hdfs dfs -get &lt;src&gt; &lt;localdst&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目录下文件数目、大小</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -count &lt;path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs –<span class="built_in">rm</span> &lt;path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -<span class="built_in">cp</span> &lt;src&gt; &lt;dest&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动文件</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -<span class="built_in">mv</span> &lt;src&gt; &lt;dest&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清空回收站（被rm的文件/文件夹）</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -expunge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除目录</span></span><br><span class="line">hadoop@hadoop1:~$  hdfs dfs -<span class="built_in">rmdir</span> &lt;path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某命令的用法</span></span><br><span class="line">hadoop@hadoop1:~$ hdfs dfs -usage &lt;<span class="built_in">command</span>&gt;</span><br></pre></td></tr></table></figure>

<p>关闭集群服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@hadoop1:~$ stop-yarn.sh</span><br><span class="line">hadoop@hadoop1:~$ stop-dfs.sh </span><br><span class="line">hadoop@hadoop1:~$ mapred --daemon stop historyserver</span><br><span class="line">hadoop@hadoop1:~$ yarn --daemon stop timelineserver</span><br></pre></td></tr></table></figure>

<p>官方指导手册链接：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">指导手册</a></p>
<h2 id="3-在Hadoop上使用MapReduce"><a href="#3-在Hadoop上使用MapReduce" class="headerlink" title="3 在Hadoop上使用MapReduce"></a>3 在Hadoop上使用MapReduce</h2><p>下面实现通过MapReduce的单词计数WordCount：<br>首先需要打开虚拟机并启动集群服务，然后再hadoop用户根目录创建软代码文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop@hadoop1:~$ vi WordCount.java</span><br></pre></td></tr></table></figure>
<p>粘贴源代码内容：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span></span><br><span class="line">       <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context</span></span><br><span class="line"><span class="params">                    )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span></span><br><span class="line">       <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span><br><span class="line"><span class="params">                       Context context</span></span><br><span class="line"><span class="params">                       )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译WordCount.java并生成jar文件（保存在~/目录中）</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop com.sun.tools.javac.Main WordCount.java</span><br><span class="line">hadoop@hadoop1:~$ jar cf wc.jar WordCount*.class</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建wordcount及输入文件夹，输出文件夹会在mapreduce执行过程中自动创建</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop fs <span class="built_in">mkdir</span> /user</span><br><span class="line">hadoop@hadoop1:~$ hadoop fs <span class="built_in">mkdir</span> /user/hadoop</span><br><span class="line">hadoop@hadoop1:~$ hadoop fs <span class="built_in">mkdir</span> /user/hadoop/wordcount</span><br><span class="line">hadoop@hadoop1:~$ hadoop fs <span class="built_in">mkdir</span> /user/hadoop/wordcount/input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将所有输入文件复制到输入文件夹中（假设文件在用户根目录）</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop fs -copyFromLocal ~/file01 /user/hadoop/wordcount/input/</span><br><span class="line">hadoop@hadoop1:~$ hadoop fs -copyFromLocal ~/file02 /user/hadoop/wordcount/input/</span><br><span class="line"><span class="comment"># 检查输入文件夹</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop fs -<span class="built_in">ls</span> /user/hadoop/wordcount/input/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行Map-Reduce操作</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop jar wc.jar WordCount /user/hadoop/wordcount/input /user/hadoop/wordcount/output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">hadoop@hadoop1:~$ hadoop fs -<span class="built_in">cat</span> /user/hadoop/wordcount/output/part-r-00000</span><br></pre></td></tr></table></figure>


<p>官方指导手册链接：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Walk-through">指导手册</a></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="/audio/破茧.mp3">
            </audio>
            
        </div>
        
    <div id="gitalk-container" class="comment link"
		data-enable="true"
        data-ae="false"
        data-ci="2ee3dab371fc6b926450"
        data-cs="80e3a063b0563b737e2a880855076d981e3d195a"
        data-r="yang-xiaoyuan.github.io"
        data-o="yang-xiaoyuan"
        data-a="*"
        data-d="false"
    >查看评论</div>


    </div>
    
        <div class="side">
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">1 Hadoop分布式存储介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4"><span class="toc-number">2.</span> <span class="toc-text">2 搭建Hadoop集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%EF%BC%88%E8%8A%82%E7%82%B9%EF%BC%89%E5%B9%B6%E9%85%8D%E7%BD%AE%E8%8A%82%E7%82%B9%E9%97%B4%E7%9A%84%E5%85%8D%E5%AF%86%E8%AE%A4%E8%AF%81"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 创建用户（节点）并配置节点间的免密认证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85Java%E5%92%8CHadoop"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 每个节点安装Java和Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E9%85%8D%E7%BD%AEHadoop"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 配置Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%90%AF%E5%8A%A8Hadoop"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 启动Hadoop</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%9C%A8Hadoop%E4%B8%8A%E4%BD%BF%E7%94%A8MapReduce"><span class="toc-number">3.</span> <span class="toc-text">3 在Hadoop上使用MapReduce</span></a></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>






</html>
